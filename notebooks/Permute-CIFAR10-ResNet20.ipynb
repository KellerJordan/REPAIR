{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021242c",
   "metadata": {},
   "source": [
    "# Permute-CIFAR10-ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd672fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), '%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabbc157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "    Cutout(12, tuple(map(int, CIFAR_MEAN))),\n",
    "]\n",
    "\n",
    "\n",
    "train_aug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=500,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/cifar_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122fde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = w*16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, w*16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w*16)\n",
    "        self.layer1 = self._make_layer(block, w*16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, w*32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, w*64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(w*64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def resnet20(w=1):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w).cuda().eval()\n",
    "\n",
    "def get_blocks(net):\n",
    "    return nn.Sequential(nn.Sequential(net.conv1, net.bn1, nn.ReLU()),\n",
    "                         *net.layer1, *net.layer2, *net.layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cacbb",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, loader=train_aug_loader):\n",
    "    n = len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(loader)):\n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t)\n",
    "            out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "            out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "            out1 = net1(img_t)\n",
    "            out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "            out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            std0_b = out0.std(dim=0)\n",
    "            std1_b = out1.std(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                std0 = torch.zeros_like(std0_b)\n",
    "                std1 = torch.zeros_like(std1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            std0 += std0_b / n\n",
    "            std1 += std1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1aae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn=None):\n",
    "    pre_weights = [conv.weight]\n",
    "    if conv.bias is not None:\n",
    "        pre_weights.append(conv.bias)\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([bn.weight, bn.bias, bn.running_mean, bn.running_var])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a convolution layer for a given\n",
    "# permutation of the input channels\n",
    "def permute_input(perm_map, conv):\n",
    "    w = conv.weight\n",
    "    w.data = w[:, perm_map, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc3136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9347, 9369)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load3(model, w, key):\n",
    "    d = '/persist/kjordan/Network-Permutations/train/resnet-batchnorm-cifar/checkpoints/'\n",
    "    p = d + 'batchnorm_resnet20x%d_e250_%s.pt' % (w, key)\n",
    "    sd = torch.load(p)\n",
    "    model.load_state_dict(sd)\n",
    "\n",
    "w_map = {\n",
    "    1: ('3dbaad51-0b9c-48a4-a7dc-f676b503e352', '7b552086-160c-4862-92f2-28b7545b8178'),\n",
    "    2: ('08f5216e-09c0-469a-9fbb-14197861c15b', '67535203-d0f4-4198-aac1-3c7f601a3936'),\n",
    "    4: ('9c0810b8-9c57-4be0-887c-02bc69ae041f', 'a8f9dc58-9539-4070-8fe5-b466c1f6f4e3'),\n",
    "    8: ('385580db-1317-4594-a10a-cddbb4661cfb', '65576a5b-eb87-4a51-baf1-bb4fa1434746'),\n",
    "    16: ('e52e0379-f115-48b6-8086-deb016abd0c8', '0432d15c-7888-4276-b9a7-17d60045a54c'),\n",
    "    32: ('376a3ce1-4c9b-4537-8a7e-2e038e461947', '2ea547fe-0459-4b7f-8a1d-eae08b9a4257'),\n",
    "}\n",
    "\n",
    "w = 1\n",
    "model0 = resnet20(w)\n",
    "model1 = resnet20(w)\n",
    "load3(model0, w, w_map[w][0])\n",
    "load3(model1, w, w_map[w][1])\n",
    "\n",
    "save_model(model0, 'batchnorm/resnet20x%d_v1' % w)\n",
    "save_model(model1, 'batchnorm/resnet20x%d_v2' % w)\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c30f8",
   "metadata": {},
   "source": [
    "### intrablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac66494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 253.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 179.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 151.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 124.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 124.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 117.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 109.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 104.85it/s]\n"
     ]
    }
   ],
   "source": [
    "blocks0 = get_blocks(model0)\n",
    "blocks1 = get_blocks(model1)\n",
    "\n",
    "for k in range(1, len(blocks1)):\n",
    "    block0 = blocks0[k]\n",
    "    block1 = blocks1[k]\n",
    "    subnet0 = nn.Sequential(blocks0[:k], block0.conv1, block0.bn1, nn.ReLU())\n",
    "    subnet1 = nn.Sequential(blocks1[:k], block1.conv1, block1.bn1, nn.ReLU())\n",
    "    perm_map = get_layer_perm(subnet0, subnet1)\n",
    "    permute_output(perm_map, block1.conv1, block1.bn1)\n",
    "    permute_input(perm_map, block1.conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33846bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model1, 'batchnorm/resnet20x%d_v2_perm1a' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50050b0f",
   "metadata": {},
   "source": [
    "### interblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008f4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(model1, 'batchnorm/resnet20x%d_v2_perm1a' % w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "945b0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 159.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 115.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 108.59it/s]\n"
     ]
    }
   ],
   "source": [
    "kk = [3, 6, 8]\n",
    "\n",
    "perm_map = get_layer_perm(blocks0[:kk[0]+1], blocks1[:kk[0]+1])\n",
    "permute_output(perm_map, model1.conv1, model1.bn1)\n",
    "for block in model1.layer1:\n",
    "    permute_input(perm_map, block.conv1)\n",
    "    permute_output(perm_map, block.conv2, block.bn2)\n",
    "block = model1.layer2[0]\n",
    "permute_input(perm_map, block.conv1)\n",
    "permute_input(perm_map, block.shortcut[0])\n",
    "\n",
    "perm_map = get_layer_perm(blocks0[:kk[1]+1], blocks1[:kk[1]+1])\n",
    "for i, block in enumerate(model1.layer2):\n",
    "    if i > 0:\n",
    "        permute_input(perm_map, block.conv1)\n",
    "    else:\n",
    "        permute_output(perm_map, block.shortcut[0], block.shortcut[1])\n",
    "    permute_output(perm_map, block.conv2, block.bn2)\n",
    "block = model1.layer3[0]\n",
    "permute_input(perm_map, block.conv1)\n",
    "permute_input(perm_map, block.shortcut[0])\n",
    "\n",
    "perm_map = get_layer_perm(blocks0[:kk[2]+1], blocks1[:kk[2]+1])\n",
    "for i, block in enumerate(model1.layer3):\n",
    "    if i > 0:\n",
    "        permute_input(perm_map, block.conv1)\n",
    "    else:\n",
    "        permute_output(perm_map, block.shortcut[0], block.shortcut[1])\n",
    "    permute_output(perm_map, block.conv2, block.bn2)\n",
    "model1.linear.weight.data = model1.linear.weight[:, perm_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c97b6e",
   "metadata": {},
   "source": [
    "### done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd69f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9369"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ffa50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model1, 'batchnorm/resnet20x%d_v2_perm1' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1):\n",
    "    sd0 = torch.load('%s.pt' % key0)\n",
    "    sd1 = torch.load('%s.pt' % key1)\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in loader:\n",
    "            output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9979caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f6264cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99.688, 0.012446842743083835, 93.47, 0.22479863911867143)\n",
      "(99.616, 0.012966994484886528, 93.69, 0.22910738438367845)\n",
      "(12.293999999999999, 2.8969798851013184, 12.53, 2.889247918128967)\n",
      "(68.726, 1.1290484881401062, 67.0, 1.2428919911384582)\n"
     ]
    }
   ],
   "source": [
    "ss = {}\n",
    "\n",
    "w = 1\n",
    "model0 = resnet20(w)\n",
    "model1 = resnet20(w)\n",
    "\n",
    "k0 = 'batchnorm/resnet20x%d_v1' % w\n",
    "k1 = 'batchnorm/resnet20x%d_v2_perm1' % w\n",
    "\n",
    "load_model(model0, k0)\n",
    "load_model(model1, k1)\n",
    "ss['model_v1'] = full_eval(model0)\n",
    "ss['model_v2'] = full_eval(model1)\n",
    "print(ss['model_v1'])\n",
    "print(ss['model_v2'])\n",
    "\n",
    "model_a = resnet20(w)\n",
    "mix_weights(model_a, 0.5, k0, k1)\n",
    "ss['permute'] = full_eval(model_a)\n",
    "print(ss['permute'])\n",
    "\n",
    "reset_bn_stats(model_a)\n",
    "ss['permute_renorm'] = full_eval(model_a)\n",
    "print(ss['permute_renorm'])\n",
    "stats['resnet20x%d' % w] = ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b37e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(stats, 'batchnorm_resnet20_barriers.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74f8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
