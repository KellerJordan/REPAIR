{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021242c",
   "metadata": {},
   "source": [
    "# Train-and-Permute-CIFAR10-VGG11-BatchNorm\n",
    "\n",
    "This is a companion notebook to `Train-and-Permute-CIFAR10-VGG11`. Here we show the simpler case of interpolating between VGGs which were trained using BatchNorm.\n",
    "\n",
    "This notebook executes the following:\n",
    "\n",
    "1. We train two 2x width VGG11 networks with BatchNorm layers, calling them model0 and model1.\n",
    "2. We compute a set of permutations which align the neurons of model1 with those of model0. We use a correlation-based method which goes back to https://arxiv.org/abs/1511.07543.\n",
    "3. We evaluate the accuracy of the interpolation between model0 and the neuron-permuted version of model1. It drops all the way down to below 60%, relative to >91% for the endpoint/parent networks.\n",
    "4. We reset the BatchNorm statistics of the interpolated network on an epoch of training data, and reevaluate. This causes the accuracy of the interpolated network to jump back up to above 90%!\n",
    "5. I.e. the \"barrier\" in terms of test accuracy goes from 91-57 = 34% to 1.23%, and in terms of test loss to 0.03.\n",
    "\n",
    "We note that the idea of resetting the BatchNorm statistics of interpolated networks goes at least back to https://arxiv.org/abs/1803.05407. In `Train-and-Permute-CIFAR10-VGG11` we investigate why this statistical reset works so well, and develop a generalization which applies even to networks trained without BatchNorm!\n",
    "\n",
    "For some comparable numbers we can look at the VGG curve of Figure 4 in Git Re-Basin (https://arxiv.org/abs/2209.04836). The authors use a permutation-only method with LayerNorm-based networks, and report a higher test loss barrier of ~0.25 for 2x-width VGG-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a directory to save VGG checkpoints\n",
    "os.makedirs('./vgg', exist_ok=True)\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), 'vgg/%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('vgg/%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabbc157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## CIFAR-10 dataloaders -- we use FFCV because it's fast\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_aug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=500,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/cifar_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation functions\n",
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b6c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.vgg_name = vgg_name\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.BatchNorm2d(self.w*x))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab3be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(w=1):\n",
    "    model = VGG('VGG11', w=w).cuda()\n",
    "    optimizer = SGD(model.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e71f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████████▋                                                                                                                                                       | 6/100 [00:14<03:48,  2.43s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = 2\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_bn_v1b' % w)\n",
    "\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_bn_v2b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cacbb",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, norm=True, loader=train_aug_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    if norm:\n",
    "        corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "        return corr\n",
    "    else:\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn):\n",
    "    pre_weights = [\n",
    "        conv.weight,\n",
    "    ]\n",
    "    if conv.bias is not None:\n",
    "        pre_weights.append(conv.bias)\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([\n",
    "            bn.weight,\n",
    "            bn.bias,\n",
    "            bn.running_mean,\n",
    "            bn.running_var,\n",
    "        ])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0db278e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9172, 9220)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = VGG('VGG11', w=w).cuda()\n",
    "model1 = VGG('VGG11', w=w).cuda()\n",
    "load_model(model0, 'vgg11x%d_bn_v1b' % w)\n",
    "load_model(model1, 'vgg11x%d_bn_v2b' % w)\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb660be",
   "metadata": {},
   "source": [
    "## Permuting neurons in model1 to match model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52945a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 115.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 105.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 90.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 69.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 64.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    layer = feats1[i]\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # get permutation and permute output of conv and maybe bn\n",
    "        if isinstance(feats1[i+1], nn.BatchNorm2d):\n",
    "            assert isinstance(feats1[i+2], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+3), subnet(model1, i+3))\n",
    "            permute_output(perm_map, feats1[i], feats1[i+1])\n",
    "        else:\n",
    "            assert isinstance(feats1[i+1], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "            permute_output(perm_map, feats1[i], None)\n",
    "        # look for succeeding layer to permute input\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(feats1[j], nn.Conv2d):\n",
    "                next_layer = feats1[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = model1.classifier\n",
    "        permute_input(perm_map, next_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b4d80",
   "metadata": {},
   "source": [
    "### Save permuted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3112ee07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9220\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11x%d_bn_v2b_perm1b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1):\n",
    "    sd0 = torch.load('vgg/%s.pt' % key0)\n",
    "    sd1 = torch.load('vgg/%s.pt' % key1)\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49939db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With neither permutation nor correction\n",
      "(α=0.00) 99.77, 0.009, 91.72, 0.295\n",
      "(α=0.25) 32.67, 1.894, 31.53, 1.922\n",
      "(α=0.50) 10.00, 2.448, 10.00, 2.448\n",
      "(α=0.75) 30.81, 1.820, 29.76, 1.860\n",
      "(α=1.00) 99.80, 0.008, 92.20, 0.301\n"
     ]
    }
   ],
   "source": [
    "model_a = VGG('VGG11', w=w).cuda()\n",
    "\n",
    "k0 = 'vgg11x%d_bn_v1b' % w\n",
    "k1 = 'vgg11x%d_bn_v2b' % w\n",
    "\n",
    "print('With neither permutation nor correction')\n",
    "for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    mix_weights(model_a, alpha, k0, k1)\n",
    "    print('(α=%.2f)' % alpha, full_eval(model_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e10d98d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With permutation but no correction\n",
      "(α=0.00) 99.77, 0.009, 91.72, 0.295\n",
      "(α=0.25) 89.00, 0.319, 82.58, 0.521\n",
      "(α=0.50) 59.93, 1.187, 58.38, 1.280\n",
      "(α=0.75) 88.87, 0.323, 82.78, 0.531\n",
      "(α=1.00) 99.80, 0.008, 92.20, 0.301\n"
     ]
    }
   ],
   "source": [
    "model_a = VGG('VGG11', w=w).cuda()\n",
    "\n",
    "k0 = 'vgg11x%d_bn_v1b' % w\n",
    "k1 = 'vgg11x%d_bn_v2b_perm1b' % w # **\n",
    "\n",
    "print('With permutation but no correction')\n",
    "for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    mix_weights(model_a, alpha, k0, k1)\n",
    "    print('(α=%.2f)' % alpha, full_eval(model_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b954910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With both permutation and correction\n",
      "(α=0.00) 99.79, 0.009, 91.73, 0.295\n",
      "(α=0.25) 99.40, 0.021, 91.64, 0.296\n",
      "(α=0.50) 97.92, 0.060, 90.74, 0.331\n",
      "(α=0.75) 99.35, 0.021, 91.74, 0.301\n",
      "(α=1.00) 99.81, 0.008, 92.21, 0.301\n"
     ]
    }
   ],
   "source": [
    "model_a = VGG('VGG11', w=w).cuda()\n",
    "\n",
    "k0 = 'vgg11x%d_bn_v1b' % w\n",
    "k1 = 'vgg11x%d_bn_v2b_perm1b' % w\n",
    "\n",
    "print('With both permutation and correction')\n",
    "for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    mix_weights(model_a, alpha, k0, k1)\n",
    "    reset_bn_stats(model_a) # **\n",
    "    print('(α=%.2f)' % alpha, full_eval(model_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23169ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e48e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
